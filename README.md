# LangGraph Research Assistant Agent

An autonomous AI agent that plans, researches, and writes professional reports on any given topic. Built with **Python** and **LangGraph**, this agent demonstrates advanced agentic workflows including state management, conditional routing, and human-in-the-loop validation.

## Overview

This project implements a multi-step research agent that breaks down complex topics into actionable search queries, gathers real-time data from the web, and synthesizes a final report. It is designed to be robust, handling failures gracefully and allowing human oversight before finalizing the output.

**Key Capabilities:**
* **Planning:** Decomposes a user topic into distinct search strategies.
* **Researching:** Uses live web search tools (DuckDuckGo or Tavily) to fetch up-to-date information.
* **Self-Correction:** Automatically retries research if initial searches yield insufficient data.
* **Human-in-the-Loop:** Pauses execution to request user approval before writing the final report.
* **Export:** Generates a formatted PDF of the final research summary.

## Architecture

The agent is built using a **StateGraph** architecture with three main nodes:

1.  **Planner Node:** Receives the topic and current date, then generates a list of targeted search queries.
2.  **Researcher Node:** Executes the search queries and aggregates findings.
3.  **Router Logic (Conditional Edge):** Checks if valid data was found.
    * *If No Data:* Loops back to the Planner for a retry (Self-Correction).
    * *If Data Found:* Proceeds to the Responder.
4.  **Responder Node:** Synthesizes the gathered data into a cohesive report with citations.

## Tech Stack

* **Python 3.10+**
* **LangGraph:** For stateful, multi-node agent orchestration.
* **LangChain:** For LLM interfacing and tool abstractions.
* **Google Gemini (Free Tier):** Primary LLM for reasoning and writing.
* **DuckDuckGo Search:** For retrieving real-time web data.
* **FPDF:** For generating the final PDF report.

## Setup and Installation
1. Clone the repository.

2. Install dependencies (It is recommended to use a virtual environment (Conda or venv).):
   ```bash
   pip install -r requirements.txt
   ```

3. Create a .env file and add your Google API key:
    ```bash
    # Free option:
    GOOGLE_API_KEY= Replace with your Google API key
    
    # Paid option:
    # OPENAI_API_KEY= Replace with your OpenAI API key
    # TAVILY_API_KEY= Replace with your Tavily API key
    ```

4. Run the Agent:
    ```bash
    python main.py
    ```

## Expectations:
1. The agent will print its research plan.
2. It will perform live web searches (you will see the queries in the console).
3. It will PAUSE and ask for your permission:
    ```bash
    Pause: Research complete. Awaiting approval. Now proceed with writing Final Report? (yes/no):
    ```
4. Type yes to authorise
5. The final report will be printed to the console and saved as Research_Report.pdf.

## Project Structure
├── main.py              # The core agent logic and graph definition
├── requirements.txt     # List of python dependencies
├── .env                 # API keys (Not uploaded to GitHub)
├── README.md            # Project documentation
└── Research_Report.pdf  # Output file generated by the agent

## Key Design Decisions
* **State Management**: Used TypedDict and MemorySaver to persist context across nodes and handle interruptions.

* **Error Handling**: Implemented a retry mechanism in the router_logic to handle cases where search tools return empty results.

* **Tooling**: Integrated a custom get_current_date() tool to ensure the agent understands the temporal context of its research.